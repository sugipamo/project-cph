# 壊れたインポート探索ロジック改善提案

## 1. 概要

壊れたインポート探索ロジック仕様書の検証結果に基づく改善提案です。実装の堅牢性と実用性を向上させるための具体的な改善案を提示します。

## 2. 探索戦略の改善

### 2.1 戦略適用条件の明確化

```python
class SearchStrategy(Protocol):
    @property
    def cost_level(self) -> int:
        """戦略のコストレベル（1:低、2:中、3:高）"""
        pass
    
    def is_applicable(self, broken_import: BrokenImport) -> bool:
        """
        戦略が適用可能かを判定
        
        判定基準を明確に定義：
        - SymbolBasedStrategy: from文でシンボルが明示されている場合
        - PathSimilarityStrategy: インポートパスが相対的に具体的な場合
        - DirectoryPatternStrategy: 特定のパターンにマッチする場合
        - GitHistoryStrategy: 他の戦略で候補が見つからない場合
        """
        pass
```

### 2.2 コストと精度のメタデータ

```python
@dataclass
class StrategyMetadata:
    name: str
    cost_level: int  # 1-3
    expected_accuracy: float  # 0.0-1.0
    timeout_seconds: float
    parallelizable: bool
    
class StrategyRegistry:
    """戦略のメタデータ管理"""
    strategies = {
        'symbol_based': StrategyMetadata(
            name='Symbol Based',
            cost_level=1,
            expected_accuracy=0.9,
            timeout_seconds=2.0,
            parallelizable=True
        ),
        'path_similarity': StrategyMetadata(
            name='Path Similarity',
            cost_level=2,
            expected_accuracy=0.7,
            timeout_seconds=5.0,
            parallelizable=True
        ),
        'git_history': StrategyMetadata(
            name='Git History',
            cost_level=3,
            expected_accuracy=0.8,
            timeout_seconds=10.0,
            parallelizable=False
        )
    }
```

## 3. スコアリングシステムの改善

### 3.1 スコア正規化

```python
class NormalizedCandidateScorer:
    """各戦略のスコアを正規化して公平に評価"""
    
    def __init__(self):
        # 各戦略のスコア分布を学習
        self.score_normalizers = {}
    
    def normalize_score(self, score: float, strategy_name: str) -> float:
        """
        戦略ごとのスコア分布に基づいて0-1に正規化
        
        方法1: パーセンタイルベース
        方法2: min-max正規化
        方法3: z-score正規化後のシグモイド変換
        """
        normalizer = self.score_normalizers.get(strategy_name)
        if normalizer:
            return normalizer.normalize(score)
        return score
    
    def update_distribution(self, strategy_name: str, scores: List[float]):
        """スコア分布を更新（オンライン学習）"""
        if strategy_name not in self.score_normalizers:
            self.score_normalizers[strategy_name] = ScoreNormalizer()
        self.score_normalizers[strategy_name].update(scores)
```

### 3.2 動的重み調整

```python
class AdaptiveWeightManager:
    """過去の成功事例から重みを学習"""
    
    def __init__(self):
        self.success_history = []
        self.current_weights = self._default_weights()
    
    def record_success(self, broken_import: BrokenImport, 
                      chosen_candidate: Candidate,
                      all_candidates: List[Candidate]):
        """成功事例を記録し、重みを更新"""
        self.success_history.append({
            'import_type': self._classify_import(broken_import),
            'winning_factors': self._analyze_winning_factors(chosen_candidate),
            'timestamp': datetime.now()
        })
        self._update_weights()
    
    def get_weights_for(self, broken_import: BrokenImport) -> Dict[str, float]:
        """インポートタイプに応じた最適な重みを返す"""
        import_type = self._classify_import(broken_import)
        return self.current_weights.get(import_type, self._default_weights())
```

## 4. Git履歴探索の実装改善

### 4.1 効率的なGit操作

```python
class OptimizedGitHistoryStrategy:
    """Git操作を最適化した履歴探索"""
    
    def __init__(self):
        self.git_cache = GitOperationCache()
    
    def track_file_movements(self, estimated_path: Path, 
                            max_depth: int) -> List[FileMovement]:
        # バッチでGit操作を実行
        movements = []
        
        # 1. 高速な名前ベース検索
        similar_files = self._find_similar_filenames(estimated_path.name)
        
        # 2. 各候補ファイルの履歴を並列で取得
        with ThreadPoolExecutor() as executor:
            futures = []
            for file_path in similar_files[:10]:  # 上位10件に制限
                future = executor.submit(self._get_file_history, file_path)
                futures.append((file_path, future))
            
            for file_path, future in futures:
                try:
                    history = future.result(timeout=5.0)
                    movements.extend(self._analyze_history(history, estimated_path))
                except TimeoutError:
                    continue
        
        return movements
    
    def _find_similar_filenames(self, filename: str) -> List[Path]:
        """ファイル名の類似性でフィルタリング（高速）"""
        # git ls-files を使用して全ファイルリストを取得
        all_files = self.git_cache.get_file_list()
        return [f for f in all_files if self._is_similar_name(f.name, filename)]
```

### 4.2 Git操作のキャッシュ

```python
class GitOperationCache:
    """Git操作結果をキャッシュして高速化"""
    
    def __init__(self, ttl_seconds: int = 300):
        self.file_list_cache = None
        self.file_list_timestamp = None
        self.history_cache = {}
        self.ttl = ttl_seconds
    
    def get_file_list(self) -> List[Path]:
        if self._is_file_list_fresh():
            return self.file_list_cache
        
        # git ls-files を実行
        result = subprocess.run(['git', 'ls-files'], 
                              capture_output=True, text=True)
        self.file_list_cache = [Path(f) for f in result.stdout.splitlines()]
        self.file_list_timestamp = time.time()
        return self.file_list_cache
```

## 5. 循環依存検出の最適化

### 5.1 インクリメンタルなグラフ更新

```python
class IncrementalDependencyGraph:
    """効率的な循環依存検出"""
    
    def __init__(self):
        self.graph = {}
        self.reverse_graph = {}
        self.scc_cache = {}  # 強連結成分のキャッシュ
    
    def would_create_cycle(self, source: Path, target: Path) -> bool:
        """
        O(V+E)ではなくO(1)～O(k)で循環を検出
        kは影響を受けるノード数
        """
        # 1. targetからsourceへの既存パスがあるかチェック
        if self._has_path_cached(target, source):
            return True
        
        # 2. 局所的な探索で確認
        return self._local_cycle_check(source, target)
    
    def _local_cycle_check(self, source: Path, target: Path, 
                          max_depth: int = 5) -> bool:
        """深さ制限付きDFSで局所的に循環をチェック"""
        visited = set()
        stack = [(target, 0)]
        
        while stack:
            node, depth = stack.pop()
            if node == source:
                return True
            if depth >= max_depth or node in visited:
                continue
            
            visited.add(node)
            for neighbor in self.graph.get(node, []):
                stack.append((neighbor, depth + 1))
        
        return False
```

## 6. エラーハンドリングの強化

### 6.1 段階的フォールバック

```python
class RobustSearchCoordinator:
    """エラーに強い探索調整"""
    
    def search_with_fallback(self, broken_import: BrokenImport) -> List[Candidate]:
        candidates = []
        errors = []
        
        for strategy_group in self._get_strategy_groups():
            try:
                group_results = self._execute_strategy_group(
                    strategy_group, broken_import
                )
                candidates.extend(group_results)
                
                # 十分な候補が見つかったら終了
                if self._has_sufficient_candidates(candidates):
                    break
                    
            except Exception as e:
                errors.append({
                    'strategy_group': strategy_group.name,
                    'error': str(e),
                    'timestamp': datetime.now()
                })
                # エラーが発生しても次のグループを試行
                continue
        
        # エラーログを記録
        if errors:
            self._log_errors(errors)
        
        return candidates
```

### 6.2 AST解析のフォールバック

```python
class ResilientASTParser:
    """AST解析失敗時のフォールバック"""
    
    def parse_module(self, file_path: Path) -> Optional[ModuleInfo]:
        # 1. 標準的なAST解析
        try:
            return self._standard_parse(file_path)
        except SyntaxError:
            pass
        
        # 2. エラー修復を試みる
        try:
            content = file_path.read_text()
            fixed_content = self._try_fix_syntax(content)
            return self._parse_string(fixed_content)
        except Exception:
            pass
        
        # 3. 正規表現ベースの簡易解析
        try:
            return self._regex_based_parse(file_path)
        except Exception:
            return None
    
    def _try_fix_syntax(self, content: str) -> str:
        """一般的な構文エラーを修復"""
        # 未完了の文字列、括弧の不一致などを修正
        fixes = [
            self._fix_unclosed_strings,
            self._fix_unmatched_brackets,
            self._fix_indentation_errors
        ]
        
        for fix_func in fixes:
            content = fix_func(content)
        
        return content
```

## 7. キャッシュ戦略の改善

### 7.1 階層的キャッシュ

```python
class HierarchicalCache:
    """メモリとディスクの2階層キャッシュ"""
    
    def __init__(self, memory_size: int = 1000, disk_path: Path = None):
        self.memory_cache = LRUCache(memory_size)
        self.disk_cache = DiskCache(disk_path or Path('.cache/import_search'))
        self.invalidation_tracker = InvalidationTracker()
    
    def get(self, key: str) -> Optional[List[Candidate]]:
        # 1. メモリキャッシュを確認
        if result := self.memory_cache.get(key):
            return result
        
        # 2. ディスクキャッシュを確認
        if result := self.disk_cache.get(key):
            # メモリキャッシュに昇格
            self.memory_cache.put(key, result)
            return result
        
        return None
    
    def invalidate_for_file(self, file_path: Path):
        """ファイル変更時の関連キャッシュ無効化"""
        affected_keys = self.invalidation_tracker.get_affected_keys(file_path)
        
        for key in affected_keys:
            self.memory_cache.remove(key)
            self.disk_cache.remove(key)
```

### 7.2 スマートな無効化

```python
class InvalidationTracker:
    """依存関係に基づくキャッシュ無効化"""
    
    def __init__(self):
        self.key_to_files = {}  # キャッシュキー -> 依存ファイル
        self.file_to_keys = {}  # ファイル -> 影響を受けるキー
    
    def track(self, cache_key: str, dependent_files: List[Path]):
        """キャッシュエントリの依存関係を記録"""
        self.key_to_files[cache_key] = dependent_files
        
        for file in dependent_files:
            if file not in self.file_to_keys:
                self.file_to_keys[file] = set()
            self.file_to_keys[file].add(cache_key)
    
    def get_affected_keys(self, changed_file: Path) -> Set[str]:
        """変更されたファイルに影響を受けるキャッシュキーを取得"""
        directly_affected = self.file_to_keys.get(changed_file, set())
        
        # 推移的な影響も考慮（オプション）
        all_affected = set(directly_affected)
        # ... 推移的な依存関係の計算
        
        return all_affected
```

## 8. 実装優先順位

1. **Phase 1（必須）**
   - スコア正規化の実装
   - 戦略適用条件の明確化
   - 基本的なエラーハンドリング

2. **Phase 2（推奨）**
   - Git操作の最適化とキャッシュ
   - 循環依存検出の効率化
   - 階層的キャッシュ

3. **Phase 3（オプション）**
   - 動的重み調整
   - AST解析のフォールバック
   - スマートな無効化

この改善提案により、より堅牢で効率的な壊れたインポート探索システムの実装が可能になります。