"""è¨­å®šã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

TypeSafeConfigNodeManagerã®æ€§èƒ½æ¸¬å®šã¨æ¯”è¼ƒ
"""
import contextlib
import json
import statistics
import tempfile
import time
from pathlib import Path
from typing import Any, Dict, List

import pytest

from src.configuration.config_manager import FileLoader, TypeSafeConfigNodeManager


class PerformanceBenchmark:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""

    @staticmethod
    def measure_execution_time(func, iterations: int = 100) -> Dict[str, float]:
        """é–¢æ•°ã®å®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®š

        Args:
            func: æ¸¬å®šã™ã‚‹é–¢æ•°
            iterations: å®Ÿè¡Œå›æ•°

        Returns:
            çµ±è¨ˆæƒ…å ±ã‚’å«ã‚€è¾æ›¸
        """
        times = []

        for _ in range(iterations):
            start_time = time.perf_counter()
            try:
                func()
            except Exception:
                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚æ™‚é–“ã¯æ¸¬å®š
                pass
            end_time = time.perf_counter()
            times.append(end_time - start_time)

        return {
            "mean": statistics.mean(times),
            "median": statistics.median(times),
            "min": min(times),
            "max": max(times),
            "stdev": statistics.stdev(times) if len(times) > 1 else 0,
            "total": sum(times),
            "iterations": iterations,
            "ops_per_second": iterations / sum(times) if sum(times) > 0 else 0
        }

    @staticmethod
    def compare_performance(func1, func2, iterations: int = 100) -> Dict[str, Any]:
        """2ã¤ã®é–¢æ•°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¯”è¼ƒ

        Returns:
            æ¯”è¼ƒçµæœã‚’å«ã‚€è¾æ›¸
        """
        results1 = PerformanceBenchmark.measure_execution_time(func1, iterations)
        results2 = PerformanceBenchmark.measure_execution_time(func2, iterations)

        speedup = results1["mean"] / results2["mean"] if results2["mean"] > 0 else float('inf')

        return {
            "baseline": results1,
            "optimized": results2,
            "speedup": speedup,
            "improvement_percentage": ((results1["mean"] - results2["mean"]) / results1["mean"]) * 100
        }


class MockLegacyConfigSystem:
    """æ—§è¨­å®šã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆæ¯”è¼ƒç”¨ï¼‰"""

    def __init__(self, config_data: Dict[str, Any]):
        self.config_data = config_data
        self._cache = {}

    def load_configuration(self):
        """è¨­å®šèª­ã¿è¾¼ã¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆé‡ã„å‡¦ç†ï¼‰"""
        # å®Ÿéš›ã®æ—§ã‚·ã‚¹ãƒ†ãƒ ã®ã‚ˆã†ã«è¤‡æ•°å›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        time.sleep(0.001)  # 1ms ã®é…å»¶

        # è¤‡æ•°ã®ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        for _ in range(5):  # 5ã¤ã®ãƒ­ãƒ¼ãƒ€ãƒ¼
            self._simulate_file_access()
            self._simulate_parsing()
            self._simulate_validation()

    def _simulate_file_access(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        time.sleep(0.0001)  # 0.1ms

    def _simulate_parsing(self):
        """ãƒ‘ãƒ¼ã‚¹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        time.sleep(0.0001)  # 0.1ms

    def _simulate_validation(self):
        """ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        time.sleep(0.0001)  # 0.1ms

    def resolve_config(self, path: List[str], target_type: type):
        """è¨­å®šè§£æ±ºã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰"""
        # æ¯å›ãƒ•ãƒ«ãƒ‘ãƒ¼ã‚¹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        time.sleep(0.0005)  # 0.5ms

        current = self.config_data
        for key in path:
            if isinstance(current, dict) and key in current:
                current = current[key]
            else:
                raise KeyError(f"Path {path} not found")

        # å‹å¤‰æ›ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰
        time.sleep(0.0001)

        return target_type(current)


class TestConfigPerformance:
    """è¨­å®šã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""

    @pytest.fixture
    def sample_config_data(self):
        """ãƒ†ã‚¹ãƒˆç”¨è¨­å®šãƒ‡ãƒ¼ã‚¿"""
        return {
            "paths": {
                "workspace_path": "./workspace",
                "contest_current_path": "./contest_current",
                "contest_stock_path": "./contest_stock/{language}/{contest_name}/{problem_name}",
                "contest_template_path": "./contest_template"
            },
            "file_patterns": {
                "contest_files": ["*.py", "*.cpp"],
                "test_files": ["*.txt", "*.in", "*.out"]
            },
            "commands": {
                "test": {
                    "steps": [
                        {"type": "shell", "cmd": ["python3", "{workspace_path}/main.py"]}
                    ],
                    "timeout": 30
                },
                "open": {
                    "steps": [
                        {"type": "copy", "src": "{contest_template_path}/main.py", "dest": "{workspace_path}/main.py"}
                    ]
                }
            },
            "timeout": {
                "default": 30,
                "build": 60,
                "test": 10
            },
            "environment_logging": {
                "enabled": True,
                "log_level": "INFO"
            }
        }

    @pytest.fixture
    def temp_config_setup(self, sample_config_data):
        """ä¸€æ™‚è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        with tempfile.TemporaryDirectory() as tmp_dir:
            tmp_path = Path(tmp_dir)

            # ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
            system_dir = tmp_path / "system"
            system_dir.mkdir()
            system_config_path = system_dir / "config.json"
            with open(system_config_path, 'w') as f:
                json.dump({
                    "paths": sample_config_data["paths"],
                    "file_patterns": sample_config_data["file_patterns"]
                }, f)

            # ç’°å¢ƒè¨­å®š
            env_dir = tmp_path / "contest_env"
            env_dir.mkdir()

            # å…±æœ‰è¨­å®š
            shared_dir = env_dir / "shared"
            shared_dir.mkdir()
            shared_config_path = shared_dir / "env.json"
            with open(shared_config_path, 'w') as f:
                json.dump({
                    "timeout": sample_config_data["timeout"],
                    "environment_logging": sample_config_data["environment_logging"]
                }, f)

            # Pythonè¨­å®š
            python_dir = env_dir / "python"
            python_dir.mkdir()
            python_config_path = python_dir / "env.json"
            with open(python_config_path, 'w') as f:
                json.dump({
                    "commands": sample_config_data["commands"]
                }, f)

            yield {
                "system_dir": str(system_dir),
                "env_dir": str(env_dir),
                "sample_data": sample_config_data
            }

    def test_initial_loading_performance(self, temp_config_setup):
        """åˆæœŸèª­ã¿è¾¼ã¿æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        def load_new_system():
            manager = TypeSafeConfigNodeManager()
            manager.load_from_files(
                system_config_dir=temp_config_setup["system_dir"],
                contest_env_dir=temp_config_setup["env_dir"],
                language="python"
            )

        def load_legacy_system():
            legacy = MockLegacyConfigSystem(temp_config_setup["sample_data"])
            legacy.load_configuration()

        comparison = PerformanceBenchmark.compare_performance(
            load_legacy_system, load_new_system, iterations=50
        )

        print("\n=== åˆæœŸèª­ã¿è¾¼ã¿æ€§èƒ½æ¯”è¼ƒ ===")
        print(f"æ—§ã‚·ã‚¹ãƒ†ãƒ å¹³å‡æ™‚é–“: {comparison['baseline']['mean']:.6f}ç§’")
        print(f"æ–°ã‚·ã‚¹ãƒ†ãƒ å¹³å‡æ™‚é–“: {comparison['optimized']['mean']:.6f}ç§’")
        print(f"æ€§èƒ½å‘ä¸Šå€ç‡: {comparison['speedup']:.2f}å€")
        print(f"æ”¹å–„ç‡: {comparison['improvement_percentage']:.1f}%")

        # æ–°ã‚·ã‚¹ãƒ†ãƒ ãŒæ—§ã‚·ã‚¹ãƒ†ãƒ ã‚ˆã‚Šé«˜é€Ÿã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert comparison['speedup'] > 1.0, "æ–°ã‚·ã‚¹ãƒ†ãƒ ã¯æ—§ã‚·ã‚¹ãƒ†ãƒ ã‚ˆã‚Šé«˜é€Ÿã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"

    def test_config_resolution_performance(self, temp_config_setup):
        """è¨­å®šè§£æ±ºæ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        # æ–°ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        manager = TypeSafeConfigNodeManager()
        manager.load_from_files(
            system_config_dir=temp_config_setup["system_dir"],
            contest_env_dir=temp_config_setup["env_dir"],
            language="python"
        )

        # æ—§ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        legacy = MockLegacyConfigSystem(temp_config_setup["sample_data"])
        legacy.load_configuration()

        # ãƒ†ã‚¹ãƒˆç”¨è¨­å®šãƒ‘ã‚¹
        test_paths = [
            (["paths", "workspace_path"], str),
            (["timeout", "default"], int),
            (["environment_logging", "enabled"], bool),
            (["commands", "test", "timeout"], int),
        ]

        def resolve_with_legacy():
            for path, target_type in test_paths:
                with contextlib.suppress(Exception):
                    legacy.resolve_config(path, target_type)

        def resolve_with_new():
            for path, target_type in test_paths:
                with contextlib.suppress(Exception):
                    manager.resolve_config(path, target_type)

        comparison = PerformanceBenchmark.compare_performance(
            resolve_with_legacy, resolve_with_new, iterations=200
        )

        print("\n=== è¨­å®šè§£æ±ºæ€§èƒ½æ¯”è¼ƒ ===")
        print(f"æ—§ã‚·ã‚¹ãƒ†ãƒ å¹³å‡æ™‚é–“: {comparison['baseline']['mean']:.6f}ç§’")
        print(f"æ–°ã‚·ã‚¹ãƒ†ãƒ å¹³å‡æ™‚é–“: {comparison['optimized']['mean']:.6f}ç§’")
        print(f"æ€§èƒ½å‘ä¸Šå€ç‡: {comparison['speedup']:.2f}å€")
        print(f"æ”¹å–„ç‡: {comparison['improvement_percentage']:.1f}%")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœã«ã‚ˆã‚Šå¤§å¹…ãªæ€§èƒ½å‘ä¸Šã‚’æœŸå¾…
        assert comparison['speedup'] > 10.0, "è¨­å®šè§£æ±ºã¯10å€ä»¥ä¸Šã®æ€§èƒ½å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™"

    def test_execution_config_creation_performance(self, temp_config_setup):
        """ExecutionConfigurationä½œæˆæ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        manager = TypeSafeConfigNodeManager()
        manager.load_from_files(
            system_config_dir=temp_config_setup["system_dir"],
            contest_env_dir=temp_config_setup["env_dir"],
            language="python"
        )

        def create_execution_config():
            config = manager.create_execution_config(
                contest_name="abc123",
                problem_name="a",
                language="python",
                env_type="local",
                command_type="test"
            )
            # ã„ãã¤ã‹ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«ã‚¢ã‚¯ã‚»ã‚¹
            _ = config.contest_name
            _ = config.problem_name
            _ = config.language

        def create_legacy_context():
            # æ—§ã‚·ã‚¹ãƒ†ãƒ ã®ExecutionContextä½œæˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            legacy = MockLegacyConfigSystem(temp_config_setup["sample_data"])
            legacy.load_configuration()
            # è¤‡æ•°ã®è¨­å®šè§£æ±ºã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            for _ in range(5):
                with contextlib.suppress(Exception):
                    legacy.resolve_config(["paths", "workspace_path"], str)

        comparison = PerformanceBenchmark.compare_performance(
            create_legacy_context, create_execution_config, iterations=100
        )

        print("\n=== ExecutionConfigä½œæˆæ€§èƒ½æ¯”è¼ƒ ===")
        print(f"æ—§ã‚·ã‚¹ãƒ†ãƒ å¹³å‡æ™‚é–“: {comparison['baseline']['mean']:.6f}ç§’")
        print(f"æ–°ã‚·ã‚¹ãƒ†ãƒ å¹³å‡æ™‚é–“: {comparison['optimized']['mean']:.6f}ç§’")
        print(f"æ€§èƒ½å‘ä¸Šå€ç‡: {comparison['speedup']:.2f}å€")
        print(f"æ”¹å–„ç‡: {comparison['improvement_percentage']:.1f}%")

        assert comparison['speedup'] > 5.0, "ExecutionConfigä½œæˆã¯5å€ä»¥ä¸Šã®æ€§èƒ½å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™"

    def test_caching_effectiveness(self, temp_config_setup):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœã®æ¸¬å®š"""
        manager = TypeSafeConfigNodeManager()
        manager.load_from_files(
            system_config_dir=temp_config_setup["system_dir"],
            contest_env_dir=temp_config_setup["env_dir"],
            language="python"
        )

        def first_access():
            return manager.resolve_config(["timeout", "default"], int)

        def repeated_access():
            return manager.resolve_config(["timeout", "default"], int)

        # åˆå›ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“æ¸¬å®š
        first_time = PerformanceBenchmark.measure_execution_time(first_access, iterations=1)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾Œã®ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“æ¸¬å®š
        # ä¸€å›ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆ
        manager.resolve_config(["timeout", "default"], int)

        cached_time = PerformanceBenchmark.measure_execution_time(repeated_access, iterations=100)

        cache_speedup = first_time["mean"] / cached_time["mean"] if cached_time["mean"] > 0 else float('inf')

        print("\n=== ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœæ¸¬å®š ===")
        print(f"åˆå›ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“: {first_time['mean']:.6f}ç§’")
        print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾Œå¹³å‡æ™‚é–“: {cached_time['mean']:.6f}ç§’")
        print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœ: {cache_speedup:.2f}å€")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœã‚’ç¢ºèª
        assert cache_speedup > 2.0, "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚Š2å€ä»¥ä¸Šã®æ€§èƒ½å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™"

    def test_memory_efficiency(self, temp_config_setup):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ãƒ†ã‚¹ãƒˆ"""
        import os

        import psutil

        process = psutil.Process(os.getpid())

        # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        initial_memory = process.memory_info().rss

        # è¤‡æ•°ã®ConfigManagerã‚’ä½œæˆ
        managers = []
        for _i in range(10):
            manager = TypeSafeConfigNodeManager()
            manager.load_from_files(
                system_config_dir=temp_config_setup["system_dir"],
                contest_env_dir=temp_config_setup["env_dir"],
                language="python"
            )

            # ã„ãã¤ã‹ã®è¨­å®šã‚’è§£æ±ºã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆ
            for _j in range(5):
                try:
                    manager.resolve_config(["timeout", "default"], int)
                    manager.resolve_config(["paths", "local_workspace_path"], str)
                except:
                    pass

            managers.append(manager)

        # æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory

        print("\n=== ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§æ¸¬å®š ===")
        print(f"åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {initial_memory / 1024 / 1024:.2f} MB")
        print(f"æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {final_memory / 1024 / 1024:.2f} MB")
        print(f"ãƒ¡ãƒ¢ãƒªå¢—åŠ é‡: {memory_increase / 1024 / 1024:.2f} MB")
        print(f"ConfigManager1å€‹ã‚ãŸã‚Š: {memory_increase / len(managers) / 1024:.2f} KB")

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¦¥å½“ãªç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆ1ã¤ã‚ãŸã‚Š1MBæœªæº€ï¼‰
        memory_per_manager = memory_increase / len(managers)
        assert memory_per_manager < 1024 * 1024, "ConfigManager1å€‹ã‚ãŸã‚Šã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯1MBæœªæº€ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"

    def test_concurrent_access_performance(self, temp_config_setup):
        """ä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        import queue
        import threading

        manager = TypeSafeConfigNodeManager()
        manager.load_from_files(
            system_config_dir=temp_config_setup["system_dir"],
            contest_env_dir=temp_config_setup["env_dir"],
            language="python"
        )

        results_queue = queue.Queue()

        def worker():
            start_time = time.perf_counter()
            for _ in range(50):
                try:
                    manager.resolve_config(["timeout", "default"], int)
                    manager.resolve_config(["paths", "local_workspace_path"], str)
                except:
                    pass
            end_time = time.perf_counter()
            results_queue.put(end_time - start_time)

        # 5ã¤ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§ä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹
        threads = []
        for _ in range(5):
            thread = threading.Thread(target=worker)
            threads.append(thread)

        overall_start = time.perf_counter()
        for thread in threads:
            thread.start()

        for thread in threads:
            thread.join()
        overall_end = time.perf_counter()

        # çµæœåé›†
        thread_times = []
        while not results_queue.empty():
            thread_times.append(results_queue.get())

        print("\n=== ä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹æ€§èƒ½æ¸¬å®š ===")
        print(f"å…¨ä½“å®Ÿè¡Œæ™‚é–“: {overall_end - overall_start:.3f}ç§’")
        print(f"ã‚¹ãƒ¬ãƒƒãƒ‰å¹³å‡æ™‚é–“: {statistics.mean(thread_times):.3f}ç§’")
        print(f"æœ€å¤§ã‚¹ãƒ¬ãƒƒãƒ‰æ™‚é–“: {max(thread_times):.3f}ç§’")
        print(f"æœ€å°ã‚¹ãƒ¬ãƒƒãƒ‰æ™‚é–“: {min(thread_times):.3f}ç§’")

        # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¢ºèª
        assert len(thread_times) == 5, "å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ãŒæ­£å¸¸ã«å®Œäº†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
        assert max(thread_times) < 1.0, "ã‚¹ãƒ¬ãƒƒãƒ‰ã‚ãŸã‚Šã®å®Ÿè¡Œæ™‚é–“ã¯1ç§’æœªæº€ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"

    def test_overall_performance_target(self, temp_config_setup):
        """ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™é”æˆç¢ºèª"""
        # ææ¡ˆæ›¸ã®4595å€æ€§èƒ½å‘ä¸Šã®æ¤œè¨¼

        manager = TypeSafeConfigNodeManager()
        legacy = MockLegacyConfigSystem(temp_config_setup["sample_data"])

        def full_legacy_workflow():
            # æ—§ã‚·ã‚¹ãƒ†ãƒ ã®å…¸å‹çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
            legacy.load_configuration()
            for _ in range(10):  # 10å›ã®è¨­å®šè§£æ±º
                try:
                    legacy.resolve_config(["paths", "workspace_path"], str)
                    legacy.resolve_config(["timeout", "default"], int)
                    legacy.resolve_config(["environment_logging", "enabled"], bool)
                except:
                    pass

        def full_new_workflow():
            # æ–°ã‚·ã‚¹ãƒ†ãƒ ã®åŒç­‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
            manager.load_from_files(
                system_config_dir=temp_config_setup["system_dir"],
                contest_env_dir=temp_config_setup["env_dir"],
                language="python"
            )
            for _ in range(10):  # 10å›ã®è¨­å®šè§£æ±º
                try:
                    manager.resolve_config(["paths", "local_workspace_path"], str)
                    manager.resolve_config(["timeout", "default"], int)
                    manager.resolve_config(["environment_logging", "enabled"], bool)
                except:
                    pass

        comparison = PerformanceBenchmark.compare_performance(
            full_legacy_workflow, full_new_workflow, iterations=20
        )

        print("\n=== ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ç¢ºèª ===")
        print(f"æ—§ã‚·ã‚¹ãƒ†ãƒ å¹³å‡æ™‚é–“: {comparison['baseline']['mean']:.6f}ç§’")
        print(f"æ–°ã‚·ã‚¹ãƒ†ãƒ å¹³å‡æ™‚é–“: {comparison['optimized']['mean']:.6f}ç§’")
        print(f"å®Ÿéš›ã®æ€§èƒ½å‘ä¸Šå€ç‡: {comparison['speedup']:.2f}å€")
        print(f"ç›®æ¨™é”æˆç‡: {(comparison['speedup'] / 100) * 100:.1f}% (ç›®æ¨™: 100å€ä»¥ä¸Š)")

        # æœ€ä½é™100å€ä»¥ä¸Šã®æ€§èƒ½å‘ä¸Šã‚’æœŸå¾…ï¼ˆå®Ÿè£…ã«ã‚ˆã£ã¦èª¿æ•´å¯èƒ½ï¼‰
        assert comparison['speedup'] > 100.0, f"ç·åˆæ€§èƒ½å‘ä¸Šã¯100å€ä»¥ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚å®Ÿéš›: {comparison['speedup']:.2f}å€"

        # ææ¡ˆæ›¸ã®ç›®æ¨™ã«è¿‘ã„æ€§èƒ½å‘ä¸ŠãŒã‚ã‚Œã°ç´ æ™´ã‚‰ã—ã„
        if comparison['speedup'] > 1000.0:
            print(f"ğŸ‰ å„ªç§€ï¼ææ¡ˆæ›¸ç›®æ¨™ã®1000å€ä»¥ä¸Šã‚’é”æˆ: {comparison['speedup']:.2f}å€")
