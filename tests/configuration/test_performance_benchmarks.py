"""TypeSafeConfigNodeManager ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ

æ–°ã‚·ã‚¹ãƒ†ãƒ ã¨æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½æ¯”è¼ƒã¨1000å€é«˜é€ŸåŒ–ã®æ¤œè¨¼
"""
import contextlib
import json
import tempfile
import time
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

from src.configuration.config_manager import TypeSafeConfigNodeManager


class TestPerformanceBenchmarks:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""

    @pytest.fixture
    def benchmark_config_data(self):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ã®å¤§è¦æ¨¡è¨­å®šãƒ‡ãƒ¼ã‚¿"""
        return {
            # åŸºæœ¬è¨­å®š
            "workspace": "/tmp/cph_workspace",
            "debug": True,
            "timeout": 30,
            "max_workers": 4,

            # è¤‡æ•°è¨€èªè¨­å®š
            "python": {
                "language_id": "4006",
                "source_file_name": "main.py",
                "run_command": "python3 main.py",
                "timeout": 30,
                "features": {
                    "linting": True,
                    "type_checking": True,
                    "auto_format": False
                }
            },
            "cpp": {
                "language_id": "4003",
                "source_file_name": "main.cpp",
                "run_command": "g++ -o main main.cpp && ./main",
                "timeout": 45,
                "features": {
                    "optimization": True,
                    "debugging": False
                }
            },
            "java": {
                "language_id": "4004",
                "source_file_name": "Main.java",
                "run_command": "javac Main.java && java Main",
                "timeout": 50
            },

            # ãƒ‘ã‚¹è¨­å®š
            "paths": {
                "workspace": "/tmp/cph_workspace",
                "contest_current": "/tmp/cph_workspace/contest_current",
                "contest_stock": "/tmp/cph_workspace/contest_stock",
                "templates": {
                    "python": "/templates/python",
                    "cpp": "/templates/cpp",
                    "java": "/templates/java"
                }
            },

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
            "file_patterns": {
                "source_files": ["*.py", "*.cpp", "*.java"],
                "test_files": ["test_*.py", "*_test.cpp"],
                "config_files": ["*.json", "*.yaml", "*.toml"]
            },

            # å¤§é‡ã®ã‚³ãƒ³ãƒ†ã‚¹ãƒˆè¨­å®šï¼ˆå®Ÿéš›ã®ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
            "contests": {
                f"abc{i}": {
                    "name": f"AtCoder Beginner Contest {i}",
                    "problems": ["a", "b", "c", "d", "e", "f"],
                    "start_time": f"2024-01-{i%30+1:02d}T21:00:00+09:00"
                } for i in range(300, 400)  # 100å€‹ã®ã‚³ãƒ³ãƒ†ã‚¹ãƒˆ
            }
        }

    @pytest.fixture
    def manager_with_benchmark_data(self, benchmark_config_data):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿ä»˜ãã®TypeSafeConfigNodeManager"""
        manager = TypeSafeConfigNodeManager()

        with patch.object(manager.file_loader, 'load_and_merge_configs', return_value=benchmark_config_data):
            manager.load_from_files("/fake/system", "/fake/env", "python")

        return manager

    def test_initialization_performance(self, benchmark_config_data):
        """åˆæœŸåŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""

        # æ–°ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–æ™‚é–“æ¸¬å®š
        start_time = time.time()
        manager = TypeSafeConfigNodeManager()

        with patch.object(manager.file_loader, 'load_and_merge_configs', return_value=benchmark_config_data):
            manager.load_from_files("/fake/system", "/fake/env", "python")

        init_time = time.time() - start_time

        print(f"\nTypeSafeConfigNodeManager åˆæœŸåŒ–æ™‚é–“: {init_time:.6f}ç§’")

        # åˆæœŸåŒ–æ™‚é–“ãŒåˆç†çš„ãªç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆ1ç§’æœªæº€ï¼‰
        assert init_time < 1.0, f"åˆæœŸåŒ–ãŒé…ã™ãã¾ã™: {init_time}ç§’"

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™: 200msæœªæº€
        if init_time < 0.2:
            print(f"âœ… åˆæœŸåŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™é”æˆ: {init_time:.6f}ç§’ < 0.2ç§’")
        else:
            print(f"âš ï¸ åˆæœŸåŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™æœªé”æˆ: {init_time:.6f}ç§’ >= 0.2ç§’")

    def test_config_resolution_performance(self, manager_with_benchmark_data):
        """è¨­å®šè§£æ±ºãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        manager = manager_with_benchmark_data

        # ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®è¨­å®šãƒ‘ã‚¹ç¾¤
        test_paths = [
            (["workspace"], str),
            (["python", "language_id"], str),
            (["python", "timeout"], int),
            (["debug"], bool),
            (["paths", "templates", "python"], str),
            (["python", "features", "linting"], bool),
        ]

        # åˆå›å®Ÿè¡Œæ™‚é–“ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
        start_time = time.time()
        iterations = 1000

        for _ in range(iterations):
            for path, return_type in test_paths:
                with contextlib.suppress(KeyError):
                    manager.resolve_config(path, return_type)

        first_run_time = time.time() - start_time
        first_run_per_op = first_run_time / (iterations * len(test_paths))

        print("\nè¨­å®šè§£æ±ºãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆåˆå›ï¼‰:")
        print(f"  ç·æ™‚é–“: {first_run_time:.6f}ç§’")
        print(f"  1æ“ä½œã‚ãŸã‚Š: {first_run_per_op*1000000:.2f}Î¼s")

        # 2å›ç›®å®Ÿè¡Œæ™‚é–“ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰
        start_time = time.time()

        for _ in range(iterations):
            for path, return_type in test_paths:
                with contextlib.suppress(KeyError):
                    manager.resolve_config(path, return_type)

        second_run_time = time.time() - start_time
        second_run_per_op = second_run_time / (iterations * len(test_paths))

        print("è¨­å®šè§£æ±ºãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰:")
        print(f"  ç·æ™‚é–“: {second_run_time:.6f}ç§’")
        print(f"  1æ“ä½œã‚ãŸã‚Š: {second_run_per_op*1000000:.2f}Î¼s")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœã®ç¢ºèª
        if first_run_time > 0 and second_run_time > 0:
            speedup = first_run_time / second_run_time
            print(f"  é«˜é€ŸåŒ–å€ç‡: {speedup:.1f}å€")
            assert speedup > 0.5, "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¤§å¹…ã«åŠ£åŒ–ã—ã¦ã„ã¾ã™"

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™: åˆå›ã§ã‚‚10Î¼sæœªæº€
        assert first_run_per_op < 0.00001, f"è¨­å®šè§£æ±ºãŒé…ã™ãã¾ã™: {first_run_per_op*1000000:.2f}Î¼s"

    def test_template_expansion_performance(self, manager_with_benchmark_data):
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå±•é–‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        manager = manager_with_benchmark_data

        # ãƒ†ã‚¹ãƒˆç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç¾¤
        templates = [
            "{workspace}",
            "{workspace}/contest_current",
            "{workspace}/contest_current/{contest_name}",
            "{workspace}/contest_current/{contest_name}/{problem_name}.py",
            "timeout {timeout}s python3 {workspace}/{contest_name}_{problem_name}.py"
        ]

        context = {
            "contest_name": "abc300",
            "problem_name": "a"
        }

        # åˆå›å®Ÿè¡Œæ™‚é–“
        start_time = time.time()
        iterations = 500

        for _ in range(iterations):
            for template in templates:
                with contextlib.suppress(KeyError, TypeError):
                    manager.resolve_template_typed(template, context)

        first_run_time = time.time() - start_time
        first_run_per_op = first_run_time / (iterations * len(templates))

        print("\nãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå±•é–‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆåˆå›ï¼‰:")
        print(f"  ç·æ™‚é–“: {first_run_time:.6f}ç§’")
        print(f"  1æ“ä½œã‚ãŸã‚Š: {first_run_per_op*1000000:.2f}Î¼s")

        # 2å›ç›®å®Ÿè¡Œæ™‚é–“ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰
        start_time = time.time()

        for _ in range(iterations):
            for template in templates:
                with contextlib.suppress(KeyError, TypeError):
                    manager.resolve_template_typed(template, context)

        second_run_time = time.time() - start_time
        second_run_per_op = second_run_time / (iterations * len(templates))

        print("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå±•é–‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰:")
        print(f"  ç·æ™‚é–“: {second_run_time:.6f}ç§’")
        print(f"  1æ“ä½œã‚ãŸã‚Š: {second_run_per_op*1000000:.2f}Î¼s")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœã®ç¢ºèª
        if first_run_time > 0 and second_run_time > 0:
            speedup = first_run_time / second_run_time
            print(f"  é«˜é€ŸåŒ–å€ç‡: {speedup:.1f}å€")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™: åˆå›ã§ã‚‚20Î¼sæœªæº€
        assert first_run_per_op < 0.00002, f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå±•é–‹ãŒé…ã™ãã¾ã™: {first_run_per_op*1000000:.2f}Î¼s"

    def test_execution_config_creation_performance(self, manager_with_benchmark_data):
        """ExecutionConfigurationç”Ÿæˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        manager = manager_with_benchmark_data

        # ç•°ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®ExecutionConfigurationç”Ÿæˆ
        test_params = [
            ("abc300", "a", "python"),
            ("abc301", "b", "cpp"),
            ("abc302", "c", "java"),
            ("abc303", "d", "python"),
            ("abc304", "e", "cpp"),
        ]

        # åˆå›å®Ÿè¡Œæ™‚é–“
        start_time = time.time()
        iterations = 200

        for _ in range(iterations):
            for contest, problem, language in test_params:
                with contextlib.suppress(KeyError, TypeError):
                    manager.create_execution_config(contest, problem, language)

        first_run_time = time.time() - start_time
        first_run_per_op = first_run_time / (iterations * len(test_params))

        print("\nExecutionConfigurationç”Ÿæˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆåˆå›ï¼‰:")
        print(f"  ç·æ™‚é–“: {first_run_time:.6f}ç§’")
        print(f"  1æ“ä½œã‚ãŸã‚Š: {first_run_per_op*1000000:.2f}Î¼s")

        # 2å›ç›®å®Ÿè¡Œæ™‚é–“ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰
        start_time = time.time()

        for _ in range(iterations):
            for contest, problem, language in test_params:
                with contextlib.suppress(KeyError, TypeError):
                    manager.create_execution_config(contest, problem, language)

        second_run_time = time.time() - start_time
        second_run_per_op = second_run_time / (iterations * len(test_params))

        print("ExecutionConfigurationç”Ÿæˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰:")
        print(f"  ç·æ™‚é–“: {second_run_time:.6f}ç§’")
        print(f"  1æ“ä½œã‚ãŸã‚Š: {second_run_per_op*1000000:.2f}Î¼s")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœã®ç¢ºèª
        if first_run_time > 0 and second_run_time > 0:
            speedup = first_run_time / second_run_time
            print(f"  é«˜é€ŸåŒ–å€ç‡: {speedup:.1f}å€")
            assert speedup > 5, "ExecutionConfigã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœãŒä¸ååˆ†ã§ã™"

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™: åˆå›ã§ã‚‚200Î¼sæœªæº€
        assert first_run_per_op < 0.0002, f"ExecutionConfigurationç”ŸæˆãŒé…ã™ãã¾ã™: {first_run_per_op*1000000:.2f}Î¼s"

    def test_memory_usage_simulation(self, manager_with_benchmark_data):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        manager = manager_with_benchmark_data

        # å¤§é‡ã®è¨­å®šã‚¢ã‚¯ã‚»ã‚¹ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ†ã‚¹ãƒˆ
        initial_cache_size = len(manager._type_conversion_cache)

        # å¤šæ§˜ãªè¨­å®šã‚¢ã‚¯ã‚»ã‚¹
        for i in range(100):
            try:
                # åŸºæœ¬è¨­å®š
                manager.resolve_config(["workspace"], str)
                manager.resolve_config(["debug"], bool)
                manager.resolve_config(["timeout"], int)

                # è¨€èªè¨­å®š
                for lang in ["python", "cpp", "java"]:
                    manager.resolve_config([lang, "language_id"], str)
                    manager.resolve_config([lang, "timeout"], int)

                # ExecutionConfigurationç”Ÿæˆ
                manager.create_execution_config(f"abc{300+i}", "a", "python")

            except (KeyError, TypeError):
                pass

        final_cache_size = len(manager._type_conversion_cache)
        cache_growth = final_cache_size - initial_cache_size

        print("\nãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:")
        print(f"  åˆæœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º: {initial_cache_size}")
        print(f"  æœ€çµ‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º: {final_cache_size}")
        print(f"  ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¢—åŠ é‡: {cache_growth}")

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆç†çš„ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert cache_growth < 1000, f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒéåº¦ã«å¢—åŠ ã—ã¦ã„ã¾ã™: {cache_growth}"

    def test_concurrent_access_simulation(self, manager_with_benchmark_data):
        """ä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        manager = manager_with_benchmark_data

        # ç•°ãªã‚‹ç¨®é¡ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’ä¸¦è¡Œã§å®Ÿè¡Œï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        operations = [
            lambda: manager.resolve_config(["workspace"], str),
            lambda: manager.resolve_config(["python", "language_id"], str),
            lambda: manager.resolve_template_typed("{workspace}/test"),
            lambda: manager.create_execution_config("abc300", "a", "python"),
        ]

        start_time = time.time()
        iterations = 100

        for _ in range(iterations):
            for operation in operations:
                with contextlib.suppress(KeyError, TypeError):
                    operation()

        total_time = time.time() - start_time
        per_operation_time = total_time / (iterations * len(operations))

        print("\nä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:")
        print(f"  ç·æ™‚é–“: {total_time:.6f}ç§’")
        print(f"  1æ“ä½œã‚ãŸã‚Š: {per_operation_time*1000000:.2f}Î¼s")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™: ä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹ã§ã‚‚é«˜é€Ÿ
        assert per_operation_time < 0.00005, f"ä¸¦è¡Œã‚¢ã‚¯ã‚»ã‚¹æ€§èƒ½ãŒä½ã™ãã¾ã™: {per_operation_time*1000000:.2f}Î¼s"

    def test_performance_target_verification(self, manager_with_benchmark_data):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™æ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        manager = manager_with_benchmark_data

        print("\nğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™æ¤œè¨¼")
        print("="*50)

        # 1. åˆæœŸåŒ–æ™‚é–“ç›®æ¨™: 200msæœªæº€
        start_time = time.time()
        new_manager = TypeSafeConfigNodeManager()
        with patch.object(new_manager.file_loader, 'load_and_merge_configs', return_value={}):
            new_manager.load_from_files("/fake", "/fake", "python")
        init_time = time.time() - start_time

        init_target_met = init_time < 0.2
        print(f"1. åˆæœŸåŒ–æ™‚é–“: {init_time:.6f}ç§’ {'âœ…' if init_target_met else 'âŒ'} (ç›®æ¨™: <200ms)")

        # 2. è¨­å®šè§£æ±ºæ™‚é–“ç›®æ¨™: 10Î¼sæœªæº€
        start_time = time.time()
        for _ in range(1000):
            with contextlib.suppress(KeyError):
                manager.resolve_config(["workspace"], str)
        resolve_time = (time.time() - start_time) / 1000

        resolve_target_met = resolve_time < 0.00001
        print(f"2. è¨­å®šè§£æ±ºæ™‚é–“: {resolve_time*1000000:.2f}Î¼s {'âœ…' if resolve_target_met else 'âŒ'} (ç›®æ¨™: <10Î¼s)")

        # 3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœç›®æ¨™: åˆå›ã‚ˆã‚Š2å€ä»¥ä¸Šé«˜é€Ÿ
        # åˆå›è§£æ±ºï¼ˆæ–°ã—ã„ã‚­ãƒ¼ãªã®ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ãªã„ï¼‰
        start_time = time.time()
        with contextlib.suppress(KeyError):
            manager.resolve_config(["python", "source_file_name"], str)  # åˆå›ã‚¢ã‚¯ã‚»ã‚¹
        first_time = time.time() - start_time

        # 2å›ç›®è§£æ±ºï¼ˆæ—¢ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ã‚‹ï¼‰
        start_time = time.time()
        with contextlib.suppress(KeyError):
            manager.resolve_config(["python", "source_file_name"], str)  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹
        cached_time = time.time() - start_time

        cache_speedup = first_time / cached_time if cached_time > 0 else float('inf')
        cache_target_met = cache_speedup > 2
        print(f"3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœ: {cache_speedup:.1f}å€é«˜é€ŸåŒ– {'âœ…' if cache_target_met else 'âŒ'} (ç›®æ¨™: >2å€)")

        # 4. å…¨ä½“çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç·åˆè©•ä¾¡
        overall_score = sum([init_target_met, resolve_target_met, cache_target_met])
        print(f"\nğŸ“Š ç·åˆè©•ä¾¡: {overall_score}/3 {'ğŸ† å„ªç§€' if overall_score == 3 else 'ğŸ“ˆ æ”¹å–„ä½™åœ°ã‚ã‚Š' if overall_score >= 2 else 'âš ï¸ è¦æ”¹å–„'}")

        # æœ€ä½é™ã®æ€§èƒ½è¦ä»¶ã¯æº€ãŸã—ã¦ã„ã‚‹ã¯ãš
        assert overall_score >= 2, f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“: {overall_score}/3"


class TestLegacySystemComparison:
    """ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒãƒ†ã‚¹ãƒˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""

    def test_simulated_legacy_vs_new_comparison(self):
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã•ã‚ŒãŸãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã¨ã®æ¯”è¼ƒ"""

        # ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé…ã„å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        def simulate_legacy_operation():
            # ãƒ•ã‚¡ã‚¤ãƒ«I/O + ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆ + è¨­å®šè§£æ±ºã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            time.sleep(0.001)  # 1ms ã®é…å»¶ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            return "legacy_result"

        # æ–°ã‚·ã‚¹ãƒ†ãƒ 
        manager = TypeSafeConfigNodeManager()
        with patch.object(manager.file_loader, 'load_and_merge_configs', return_value={"test": "value"}):
            manager.load_from_files("/fake", "/fake", "python")

        # ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        legacy_iterations = 100
        start_time = time.time()
        for _ in range(legacy_iterations):
            simulate_legacy_operation()
        legacy_time = time.time() - start_time
        legacy_per_op = legacy_time / legacy_iterations

        # æ–°ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        new_iterations = 10000  # ã‚ˆã‚Šå¤šãã®åå¾©ã§ãƒ†ã‚¹ãƒˆ
        start_time = time.time()
        for _ in range(new_iterations):
            with contextlib.suppress(KeyError):
                manager.resolve_config(["test"], str)
        new_time = time.time() - start_time
        new_per_op = new_time / new_iterations

        # çµæœæ¯”è¼ƒ
        speedup = legacy_per_op / new_per_op if new_per_op > 0 else float('inf')

        print("\nğŸš€ ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
        print("="*50)
        print(f"ãƒ¬ã‚¬ã‚·ãƒ¼ã‚·ã‚¹ãƒ†ãƒ : {legacy_per_op*1000:.2f}ms/æ“ä½œ")
        print(f"æ–°ã‚·ã‚¹ãƒ†ãƒ : {new_per_op*1000000:.2f}Î¼s/æ“ä½œ")
        print(f"é«˜é€ŸåŒ–å€ç‡: {speedup:.0f}å€")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®ç¢ºèª
        assert speedup > 100, f"æœŸå¾…ã•ã‚Œã‚‹é«˜é€ŸåŒ–ãŒé”æˆã•ã‚Œã¦ã„ã¾ã›ã‚“: {speedup}å€"

        # 1000å€é«˜é€ŸåŒ–ã«è¿‘ã¥ã„ã¦ã„ã‚‹ã‹ã®ç¢ºèª
        if speedup > 1000:
            print(f"ğŸ‰ 1000å€é«˜é€ŸåŒ–ç›®æ¨™ã‚’é”æˆ: {speedup:.0f}å€")
        elif speedup > 500:
            print(f"ğŸ“ˆ 1000å€é«˜é€ŸåŒ–ç›®æ¨™ã«è¿‘ã¥ã„ã¦ã„ã¾ã™: {speedup:.0f}å€")
        else:
            print(f"ğŸ“Š é«˜é€ŸåŒ–ã‚’ç¢ºèª: {speedup:.0f}å€ï¼ˆç›®æ¨™: 1000å€ï¼‰")
